#include <cassert>
#include <cstddef>
#include <cstdint>
#include <limits>
#include <memory>
#include <sstream>
#include <string>
#include <vector>

#include <cuda_runtime.h>

#include "AC/Core/Image.hpp"
#include "AC/Core/Model.hpp"
#include "AC/Core/Processor.hpp"
#include "AC/Core/Util.hpp"
#include "AC/Util/ThreadLocal.hpp"

#include "ACCoreExport.hpp" // Generated by CMake

#define ContextList (ac::core::cuda::getContextList())

namespace ac::core::cuda
{
    void conv3x3_1to8_relu_cuda(
        const void* sptr,
        int srcW, int srcH, int srcC, int spitch,
        void* dptr,
        int dstW, int dstH, int dstC, int dpitch,
        const float* kernels,
        const float* biases,
        Image::ElementType stype,
        cudaStream_t stream
    ) noexcept;
    void conv3x3_8to8_relu_cuda(
        const void* sptr,
        int srcW, int srcH, int srcC, int spitch,
        void* dptr,
        int dstW, int dstH, int dstC, int dpitch,
        const float* kernels,
        const float* biases,
        cudaStream_t stream
    ) noexcept;
    void deconv2x2_8to1_cuda(
        const void* sptr,
        int srcW, int srcH, int srcC, int spitch,
        void* dptr,
        int dstW, int dstH, int dstC, int dpitch,
        const float* kernels,
        Image::ElementType dtype,
        cudaStream_t stream
    ) noexcept;
    void conv3x3_1to8_identity_cuda(
        const void* sptr,
        int srcW, int srcH, int srcC, int spitch,
        void* dptr,
        int dstW, int dstH, int dstC, int dpitch,
        const float* kernels,
        const float* biases,
        Image::ElementType stype,
        cudaStream_t stream
    ) noexcept;
    void conv3x3_8to8_lrelu_cuda(
        const void* sptr,
        int srcW, int srcH, int srcC, int spitch,
        void* dptr,
        int dstW, int dstH, int dstC, int dpitch,
        const float* kernels,
        const float* biases,
        const float negativeSlope,
        cudaStream_t stream
    ) noexcept;
    void conv3x3_8to8_residual_identity_cuda(
        const void* sptr,
        int srcW, int srcH, int srcC, int spitch,
        void* dptr,
        int dstW, int dstH, int dstC, int dpitch,
        const float* kernels,
        const float* biases,
        void* iptr,
        int idW, int idH, int idC, int ipitch,
        const float scale,
        cudaStream_t stream
    ) noexcept;
    void conv3x3_8to8_residual_identity_cuda(
        const void* sptr,
        int srcW, int srcH, int srcC, int spitch,
        void* dptr,
        int dstW, int dstH, int dstC, int dpitch,
        const float* kernels,
        const float* biases,
        void* iptr,
        int idW, int idH, int idC, int ipitch,
        const float scale,
        void* fptr,
        int featW, int featH, int featC, int fpitch,
        cudaStream_t stream
    ) noexcept;
    void conv3x3_8to4_identity_pixelshuffle_4to1_cuda(
        const void* sptr,
        int srcW, int srcH, int srcC, int spitch,
        void* dptr,
        int dstW, int dstH, int dstC, int dpitch,
        const float* kernels,
        const float* biases,
        Image::ElementType dtype,
        cudaStream_t stream
    ) noexcept;

    struct Context
    {
        std::string name{};
        int vram{};
        int computeCapability{};
        bool memoryPoolsSupported{};

        Context(const cudaDeviceProp& deviceProp) noexcept
        {
            name = deviceProp.name;
            vram = static_cast<int>(deviceProp.totalGlobalMem >> 20);
            computeCapability = deviceProp.major * 10 + deviceProp.minor;
            memoryPoolsSupported = deviceProp.memoryPoolsSupported;
        }
    };

    //lazy load, god knows if it's safe to call the cuda function during DLL initialization
    static inline std::vector<Context>& getContextList() noexcept
    {
        static auto contextList = []() -> std::vector<Context> {
            int deviceCount = 0;
            auto err = cudaGetDeviceCount(&deviceCount); if (err != cudaSuccess) return {};
            std::vector<Context> contexts{};
            for (int i = 0; i < deviceCount; i++)
            {
                cudaDeviceProp deviceProp{};
                err = cudaGetDeviceProperties(&deviceProp, i); if (err != cudaSuccess) continue;
                contexts.emplace_back(deviceProp);
            }
            return contexts;
        }();
        return contextList;
    }

    struct DeviceImage
    {
        void* ptr = nullptr;
        int w = 0;
        int h = 0;
        int c = 0;
        int pitch = 0;
        int elementSize = 0;

        void create(const int w, const int h, const int c, const int elementSize) noexcept
        {
            this->w = w;
            this->h = h;
            this->c = c;
            this->elementSize = elementSize;
            pitch = align(w * c * elementSize, 128);
        }

        cudaError_t fromHost(const Image& hostImage, const cudaStream_t stream) const noexcept
        {
            auto lineSize = hostImage.width() * hostImage.pixelSize();
            return cudaMemcpy2DAsync(ptr, pitch, hostImage.ptr(), hostImage.stride(), lineSize, hostImage.height(), cudaMemcpyHostToDevice, stream);
        }
        cudaError_t toHost(Image& hostImage, const cudaStream_t stream) const noexcept
        {
            auto lineSize = w * c * elementSize;
            return cudaMemcpy2DAsync(hostImage.ptr(), hostImage.stride(), ptr, pitch, lineSize, h, cudaMemcpyDeviceToHost, stream);
        }
    };

    class DeviceImageAllocator
    {
    public:
        cudaError_t init(const int idx) noexcept
        {
            cudaError_t err = cudaSuccess;

            device = idx;
            memoryPoolsSupported = ContextList[idx].memoryPoolsSupported;
            if (memoryPoolsSupported)
            {
                deviceMalloc = deviceMallocAsync;
                deviceFree = deviceFreeAsync;

                cudaMemPool_t pool{};
                err = cudaDeviceGetDefaultMemPool(&pool, device);
                if (err == cudaSuccess)
                {
                    auto threshold = std::numeric_limits<std::uint64_t>::max();
                    err = cudaMemPoolSetAttribute(pool, cudaMemPoolAttrReleaseThreshold, &threshold);
                }
            }
            else
            {
                deviceMalloc = deviceMallocSync;
                deviceFree = deviceFreeSync;
            }

            return err;
        }
        cudaError_t deinit() const noexcept
        {
            cudaError_t err = cudaSuccess;

            if (memoryPoolsSupported)
            {
                cudaMemPool_t pool{};
                err = cudaDeviceGetDefaultMemPool(&pool, device);
                if (err == cudaSuccess) err = cudaMemPoolTrimTo(pool, 0);
            }

            return err;
        }

        cudaError_t allocate(DeviceImage& buffer, const cudaStream_t stream) const noexcept
        {
            auto size = buffer.pitch * buffer.h;
            return deviceMalloc(&buffer.ptr, size, stream);
        }
        cudaError_t deallocate(DeviceImage& buffer, const cudaStream_t stream) const noexcept
        {
            auto err = cudaSuccess;
            if (buffer.ptr)
            {
                err = deviceFree(buffer.ptr, stream);
                if (err == cudaSuccess) buffer.ptr = nullptr;
            }
            return err;
        }

    private:
        static cudaError_t deviceMallocAsync(void** const devPtr, const std::size_t size, const cudaStream_t stream) noexcept
        {
            return cudaMallocAsync(devPtr, size, stream);
        }
        static cudaError_t deviceFreeAsync(void* const devPtr, const cudaStream_t stream) noexcept
        {
            return cudaFreeAsync(devPtr, stream);
        }
        static cudaError_t deviceMallocSync(void** const devPtr, const std::size_t size, const cudaStream_t /*stream*/) noexcept
        {
            return cudaMalloc(devPtr, size);
        }
        static cudaError_t deviceFreeSync(void* const devPtr, const cudaStream_t /*stream*/) noexcept
        {
            return cudaFree(devPtr);
        }

    private:
        decltype(&deviceMallocAsync) deviceMalloc = deviceMallocSync;
        decltype(&deviceFreeAsync) deviceFree = deviceFreeSync;
        int device = 0;
        bool memoryPoolsSupported = false;
    };

    class ImageBuffer
    {
    public:
        ImageBuffer() noexcept = default;
        ImageBuffer(const ImageBuffer&) = delete;
        ImageBuffer(ImageBuffer&& other) = delete;
        ImageBuffer& operator=(const ImageBuffer&) = delete;
        ImageBuffer& operator=(ImageBuffer&& other) = delete;
        ~ImageBuffer() noexcept
        {
            if (buffer.ptr)
            {   // There should not be any errors here.
                auto err = cudaFree(buffer.ptr);
                if (err == cudaSuccess) buffer.ptr = nullptr;
                assert(err == cudaSuccess);
            }
        }

        const DeviceImage& get(
            const int width,
            const int height,
            const int channels,
            const int elementSize,
            const DeviceImageAllocator& allocator,
            const cudaStream_t stream,
            cudaError_t& err) noexcept
        {
            if (!buffer.ptr || buffer.w != width || buffer.h != height || buffer.elementSize != elementSize)
            {
                if (buffer.ptr)
                {
                    err = allocator.deallocate(buffer, stream);
                    if (err != cudaSuccess) return buffer;
                }
                // update shapes
                buffer.create(width, height, channels, elementSize);

                err = allocator.allocate(buffer, stream);
                if (err != cudaSuccess) buffer = {};
            }

            return buffer;
        }

    private:
        DeviceImage buffer{};
    };

    class CUDAProcessorBase : public Processor
    {
    public:
        CUDAProcessorBase(const int device) noexcept
        {
            auto& err = errors.local();
            if (ContextList.empty()) err = cudaErrorNoDevice;
            else
            {
                idx = (device >= 0 && static_cast<decltype(ContextList.size())>(device) < ContextList.size()) ? device : 0;
                err = allocator.init(idx);
            }
        };
        ~CUDAProcessorBase() noexcept override
        {
            // there should not be any error here, we tentatively keep 'err' to avoid ignoring the return value.
            auto& err = errors.local();
            err = allocator.deinit();
            assert(err == cudaSuccess);
        }

        bool ok() noexcept override
        {
            return errors.local() == cudaSuccess;
        }
        const char* error() noexcept override
        {
            return cudaGetErrorString(errors.local());
        }
        const char* name() const noexcept override
        {
            return ContextList[idx].name.c_str();
        }
        int type() const noexcept override
        {
            return Processor::CUDA;
        }
        const char* typeName() const noexcept override
        {
            return "CUDA";
        }
    protected:
        DeviceImageAllocator allocator{};
        util::ThreadLocal<cudaError_t> errors{};
    };

    template<typename Model>
    class CUDAProcessorSeqCNN : public CUDAProcessorBase
    {
    public:
        CUDAProcessorSeqCNN(const int device, const Model& model) noexcept : CUDAProcessorBase(device), model(model)
        {
            auto& err = errors.local(); if (err != cudaSuccess) return;
            err = cudaSetDevice(idx); if (err != cudaSuccess) return;

            for (int i = 0; i < model.kernels(); i++)
            {
                float* dkptr = nullptr;
                auto hkptr = model.kernel(i);
                auto size = model.kernelSize(i);
                err = cudaMalloc(&dkptr, size); if (err != cudaSuccess) return;
                kernels.emplace_back(dkptr);
                err = cudaMemcpy(dkptr, hkptr, size, cudaMemcpyHostToDevice); if (err != cudaSuccess) return;
            }
            for (int i = 0; i < model.biases(); i++)
            {
                float* dkptr = nullptr;
                auto hkptr = model.bias(i);
                auto size = model.biasSize(i);
                err = cudaMalloc(&dkptr, size); if (err != cudaSuccess) return;
                biases.emplace_back(dkptr);
                err = cudaMemcpy(dkptr, hkptr, size, cudaMemcpyHostToDevice); if (err != cudaSuccess) return;
            }
        }
        ~CUDAProcessorSeqCNN() noexcept override
        {
            // there should not be any error here, we tentatively keep 'err' to avoid ignoring the return value.
            auto& err = errors.local();
            err = cudaSetDevice(idx);

            for(auto kernel : kernels) err = cudaFree(kernel);
            for(auto bias : biases) err = cudaFree(bias);
        }

    protected:
        Model model;
        std::vector<float*> kernels{};
        std::vector<float*> biases{};
    };

    template<typename Model>
    class CUDAProcessor;
}

template<>
class ac::core::cuda::CUDAProcessor<ac::core::model::ACNet> : public CUDAProcessorSeqCNN<model::ACNet>
{
public:
    CUDAProcessor(int device, const model::ACNet& model) noexcept;
    ~CUDAProcessor() noexcept override;

private:
    void process(const Image& src, Image& dst) override;

private:
    util::ThreadLocal<ImageBuffer> inImageBuffers{};
    util::ThreadLocal<ImageBuffer> tmp1ImageBuffers{};
    util::ThreadLocal<ImageBuffer> tmp2ImageBuffers{};
    util::ThreadLocal<ImageBuffer> outImageBuffers{};
};

ac::core::cuda::CUDAProcessor<ac::core::model::ACNet>::CUDAProcessor(const int device, const model::ACNet& model) noexcept : CUDAProcessorSeqCNN(device, model) {}
ac::core::cuda::CUDAProcessor<ac::core::model::ACNet>::~CUDAProcessor() noexcept = default;

void ac::core::cuda::CUDAProcessor<ac::core::model::ACNet>::process(const Image& src, Image& dst)
{
    auto& err = errors.local();
    auto stream = cudaStreamPerThread;

    err = cudaSetDevice(idx); if (err != cudaSuccess) return;

    auto& inImageBuffer = inImageBuffers.local();
    auto& tmp1ImageBuffer = tmp1ImageBuffers.local();
    auto& tmp2ImageBuffer = tmp2ImageBuffers.local();
    auto& outImageBuffer = outImageBuffers.local();

    auto& in = inImageBuffer.get(src.width(), src.height(), src.channels(), src.elementSize(), allocator, stream, err); if (err != cudaSuccess) return;
    auto& tmp1 = tmp1ImageBuffer.get(src.width(), src.height(), 8, 2, allocator, stream, err); if (err != cudaSuccess) return;
    auto& tmp2 = tmp2ImageBuffer.get(src.width(), src.height(), 8, 2, allocator, stream, err); if (err != cudaSuccess) return;
    auto& out = outImageBuffer.get(dst.width(), dst.height(), dst.channels(), dst.elementSize(), allocator, stream, err); if (err != cudaSuccess) return;

    int l = 0;

    err = in.fromHost(src, stream); if (err != cudaSuccess) return;

    conv3x3_1to8_relu_cuda(
        in.ptr, in.w, in.h, in.c, in.pitch,
        tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
        kernels[l], biases[l],
        src.type(), stream); l++;

    for (int i = 0; i < 4; i++)
    {
        conv3x3_8to8_relu_cuda(
            tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
            tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch,
            kernels[l], biases[l],
            stream); l++;

        conv3x3_8to8_relu_cuda(
            tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch,
            tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
            kernels[l], biases[l],
            stream); l++;
    }

    deconv2x2_8to1_cuda(
        tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
        out.ptr, out.w, out.h, out.c, out.pitch,
        kernels[l],
        dst.type(), stream);

    err = cudaPeekAtLastError(); if (err != cudaSuccess) return; // check any launch error.

    err = out.toHost(dst, stream); if (err != cudaSuccess) return;

    err = cudaStreamSynchronize(stream); if (err != cudaSuccess) return;

    err = cudaPeekAtLastError();
}

template<>
AC_CORE_EXPORT std::shared_ptr<ac::core::Processor> ac::core::Processor::create<ac::core::Processor::CUDA, ac::core::model::ACNet>(const int idx, const model::ACNet& model)
{
    return std::make_shared<cuda::CUDAProcessor<model::ACNet>>(idx, model);
}


template<>
class ac::core::cuda::CUDAProcessor<ac::core::model::ARNet> : public CUDAProcessorSeqCNN<model::ARNet>
{
public:
    CUDAProcessor(int device, const model::ARNet& model) noexcept;
    ~CUDAProcessor() noexcept override;

private:
    void process(const Image& src, Image& dst) override;

private:
    util::ThreadLocal<ImageBuffer> inImageBuffers{};
    util::ThreadLocal<ImageBuffer> tmp1ImageBuffers{};
    util::ThreadLocal<ImageBuffer> tmp2ImageBuffers{};
    util::ThreadLocal<ImageBuffer> featImageBuffers{};
    util::ThreadLocal<ImageBuffer> outImageBuffers{};
};

ac::core::cuda::CUDAProcessor<ac::core::model::ARNet>::CUDAProcessor(const int device, const model::ARNet& model) noexcept : CUDAProcessorSeqCNN(device, model) {}
ac::core::cuda::CUDAProcessor<ac::core::model::ARNet>::~CUDAProcessor() noexcept = default;

void ac::core::cuda::CUDAProcessor<ac::core::model::ARNet>::process(const Image& src, Image& dst)
{
    auto& err = errors.local();
    auto stream = cudaStreamPerThread;

    err = cudaSetDevice(idx); if (err != cudaSuccess) return;

    auto& inImageBuffer = inImageBuffers.local();
    auto& tmp1ImageBuffer = tmp1ImageBuffers.local();
    auto& tmp2ImageBuffer = tmp2ImageBuffers.local();
    auto& featImageBuffer = featImageBuffers.local();
    auto& outImageBuffer = outImageBuffers.local();

    auto& in = inImageBuffer.get(src.width(), src.height(), src.channels(), src.elementSize(), allocator, stream, err); if (err != cudaSuccess) return;
    auto& tmp1 = tmp1ImageBuffer.get(src.width(), src.height(), 8, 2, allocator, stream, err); if (err != cudaSuccess) return;
    auto& tmp2 = tmp2ImageBuffer.get(src.width(), src.height(), 8, 2, allocator, stream, err); if (err != cudaSuccess) return;
    auto& feat = featImageBuffer.get(src.width(), src.height(), 8, 2, allocator, stream, err); if (err != cudaSuccess) return;
    auto& out = outImageBuffer.get(dst.width(), dst.height(), dst.channels(), dst.elementSize(), allocator, stream, err); if (err != cudaSuccess) return;

    int l = 0;

    err = in.fromHost(src, stream); if (err != cudaSuccess) return;

    conv3x3_1to8_identity_cuda(
        in.ptr, in.w, in.h, in.c, in.pitch,
        feat.ptr, feat.w, feat.h, feat.c, feat.pitch,
        kernels[l], biases[l],
        src.type(), stream); l++;

    conv3x3_8to8_lrelu_cuda(
        feat.ptr, feat.w, feat.h, feat.c, feat.pitch,
        tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
        kernels[l], biases[l], 0.2f, stream); l++;
    conv3x3_8to8_residual_identity_cuda(
        tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
        tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch,
        kernels[l], biases[l],
        feat.ptr, feat.w, feat.h, feat.c, feat.pitch, 0.2f, stream); l++;
    for (int i = 0; i < model.blocks() - 2; i++)
    {
        conv3x3_8to8_lrelu_cuda(
            tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch,
            tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
            kernels[l], biases[l], 0.2f, stream); l++;
        conv3x3_8to8_residual_identity_cuda(
            tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
            tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch,
            kernels[l], biases[l],
            tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch, 0.2f, stream); l++;
    }
    conv3x3_8to8_lrelu_cuda(
        tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch,
        tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
        kernels[l], biases[l], 0.2f, stream); l++;
    conv3x3_8to8_residual_identity_cuda(
        tmp1.ptr, tmp1.w, tmp1.h, tmp1.c, tmp1.pitch,
        tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch,
        kernels[l], biases[l],
        tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch, 0.2f,
        feat.ptr, feat.w, feat.h, feat.c, feat.pitch,
        stream); l++;
    conv3x3_8to4_identity_pixelshuffle_4to1_cuda(
        tmp2.ptr, tmp2.w, tmp2.h, tmp2.c, tmp2.pitch,
        out.ptr, out.w, out.h, out.c, out.pitch,
        kernels[l], biases[l],
        dst.type(), stream);

    err = cudaPeekAtLastError(); if (err != cudaSuccess) return;

    err = out.toHost(dst, stream); if (err != cudaSuccess) return;

    err = cudaStreamSynchronize(stream); if (err != cudaSuccess) return;

    err = cudaPeekAtLastError();
}

template<>
AC_CORE_EXPORT std::shared_ptr<ac::core::Processor> ac::core::Processor::create<ac::core::Processor::CUDA, ac::core::model::ARNet>(const int idx, const model::ARNet& model)
{
    return std::make_shared<cuda::CUDAProcessor<model::ARNet>>(idx, model);
}


template<>
AC_CORE_EXPORT const char* ac::core::Processor::info<ac::core::Processor::CUDA>()
{
    static auto infoBuffer = []() -> std::string {
        std::ostringstream buffer{ "CUDA:\n", std::ios_base::ate };
        for (int i = 0; i < ContextList.size(); i++)
            buffer << "  [" << i << "] " << ContextList[i].name << " (" << ContextList[i].vram << "MB, CC " << ContextList[i].computeCapability / 10.0 << ")" << '\n';
        return buffer.str();
    }();
    return infoBuffer.c_str();
}
